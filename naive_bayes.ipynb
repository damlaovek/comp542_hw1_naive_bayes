{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 92.704%\n"
     ]
    }
   ],
   "source": [
    "# Read data and store in arrays\n",
    "## In order for the data to be read correctly, this notebook and aclImdb folder which contains \n",
    "## training and test data should be in the same directory.\n",
    "function read_data(path)\n",
    "    contents = String[]\n",
    "    for (root, dirs, files) in walkdir(path)\n",
    "        push!.(Ref(contents), read.(joinpath.(root, files), String))\n",
    "    end\n",
    "    contents\n",
    "end\n",
    "\n",
    "## First the positive training examples and then, the negative ones\n",
    "trn_pos = read_data(\"aclImdb/train/pos\")\n",
    "trn_neg = read_data(\"aclImdb/train/neg\")\n",
    "\n",
    "## Remove punctuations, html <br > and special characters\n",
    "## Convert to lowercase\n",
    "function preprocess(text; remove_punc=true)\n",
    "    text = lowercase(text)\n",
    "    if remove_punc\n",
    "        punc_and_spec = ['\"','!','\\'','^','#','+','%','&','/','(',')','[',']','{','}','=','*','?','@',';',',',':','.','\\\\','|']\n",
    "        text = replace(text, punc_and_spec => \"\")\n",
    "    end\n",
    "    text = replace(text, \"<br >\" => \" \")\n",
    "    text = replace(text, ['-','_'] => \" \")\n",
    "    text\n",
    "end\n",
    "\n",
    "## Preprocess training data\n",
    "trn_pos = preprocess.(trn_pos; remove_punc=false)\n",
    "trn_neg = preprocess.(trn_neg)\n",
    "\n",
    "## Tokenize the texts into individual words\n",
    "function tokenize(text)\n",
    "    tokens = split(text)\n",
    "end\n",
    "\n",
    "## Create dictionaries in the form of (word => frequency)\n",
    "function calculate_freq(iter)\n",
    "    word_dict = Dict{String, Integer}()\n",
    "    for i in iter\n",
    "        if haskey(word_dict, i)\n",
    "            freq = get(word_dict, i, 1)\n",
    "            word_dict[i] = freq + 1\n",
    "        else\n",
    "            word_dict[i] = 1\n",
    "        end\n",
    "    end\n",
    "    word_dict\n",
    "end\n",
    "\n",
    "## Parse training data into words and calculate  word frequencies\n",
    "trn_pos_tokens = tokenize.(trn_pos)\n",
    "trn_pos_tokens = collect(Iterators.flatten(trn_pos_tokens))\n",
    "trn_pos_words = calculate_freq(trn_pos_tokens)\n",
    "trn_neg_tokens = tokenize.(trn_neg)\n",
    "trn_neg_tokens = collect(Iterators.flatten(trn_neg_tokens))\n",
    "trn_neg_words = calculate_freq(trn_neg_tokens)\n",
    "\n",
    "# Read test data & preprocess\n",
    "test_pos = read_data(\"aclImdb/test/pos\")\n",
    "test_neg = read_data(\"aclImdb/test/neg\")\n",
    "test_pos = preprocess.(test_pos; remove_punc=false)\n",
    "test_neg = preprocess.(test_neg)\n",
    "\n",
    "# Eliminate words with low frequency\n",
    "## Threshold values have been optimized by trial and error strategy\n",
    "threshold_pos = sum(values(trn_pos_words))/length(values(trn_pos_words)) * 3\n",
    "trn_pos_words = filter(p->(last(p)>=threshold_pos), trn_pos_words)\n",
    "\n",
    "threshold_neg = sum(values(trn_neg_words))/length(values(trn_neg_words)) * 2.2\n",
    "trn_neg_words = filter(p->(last(p)>=threshold_neg), trn_neg_words)\n",
    "\n",
    "# Total count of all features in the training set\n",
    "d = length(unique(collect(Iterators.flatten(vcat(keys(trn_pos_words), keys(trn_neg_words))))))\n",
    "\n",
    "# Total count of all features in class 'positive'\n",
    "total_cnts_features_p = length(unique(keys(trn_pos_words)))\n",
    "\n",
    "# Total count of all features in class 'negative'\n",
    "total_cnts_features_n = length(unique(keys(trn_neg_words)))\n",
    "\n",
    "# Calculate P(word|class)\n",
    "function calculate_word_prob(word, dictionary, total_cnts_features, d, alpha)\n",
    "    if haskey(dictionary, word)\n",
    "        prob = (dictionary[word] + alpha)/(total_cnts_features + alpha*d)\n",
    "    else\n",
    "        prob = alpha/(total_cnts_features + alpha*d)\n",
    "    end\n",
    "    prob\n",
    "end\n",
    "\n",
    "# Predict class of a sentence\n",
    "function naive_bayes_classifier(sentence; alpha = 1)\n",
    "    prob_pos = Float64(1)\n",
    "    prob_neg = Float64(1)\n",
    "    tokens = tokenize(sentence)\n",
    "    for token in tokens\n",
    "        prob_pos *= calculate_word_prob(token, trn_pos_words, total_cnts_features_p, d, alpha)\n",
    "        prob_neg *= calculate_word_prob(token, trn_neg_words, total_cnts_features_n, d, alpha)\n",
    "    end\n",
    "    if prob_pos >= prob_neg\n",
    "        pred = 1\n",
    "    else\n",
    "        pred = 0\n",
    "    end\n",
    "    pred\n",
    "end\n",
    "\n",
    "\n",
    "############## MAIN ###############################\n",
    "\n",
    "## Run Naive Bayes Classifier\n",
    "pred_pos_test = naive_bayes_classifier.(test_pos)\n",
    "pred_neg_test = naive_bayes_classifier.(test_neg)\n",
    "\n",
    "## Calculate Accuracy\n",
    "total = length(pred_pos_test) + length(pred_neg_test)\n",
    "true_pos = length(filter(e-> e==1,pred_pos_test))\n",
    "true_neg = length(filter(e-> e==0,pred_neg_test))\n",
    "accuracy = (true_pos + true_neg)/total * 100\n",
    "println(\"Accuracy is: $accuracy%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
